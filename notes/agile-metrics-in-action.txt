p.4: Feedback-Loop: Collect - Measure - React - Repeat
p.5: Was sind Metriken? Eine Methode etwas zu messen oder deren Ergebisse.
  Start bei: welche Metriken haben wir bereits? Häufig sind Velocity oder CLOC (Changed Lines of Code)
p.6: Probleme:
  - agile Definition von Messungen ist nicht direkt gegeben (Wie messe ich "working software"?)
  - agile fokussiert sich auf ein Produkt, nicht ein Projekt
  - Daten sind über den Entwicklungsprozess verstreut, ohne einheitliche Sicht
p.9: Data:
  - Project Mnagement: Story Points per Sprint, fertiggestellte Tasks, neue Bugs
  - Source Control: Anzahl Änderungen im Code, Reviews, Kommentare
  - Build System: statische Code Analyse, Testabdeckung, fehlerhafte Builds
  - System Überwachung
p.13 Analyzing Data:
  - Herausfinden was relevant ist: z.B. über MindMap
  - Visualisierung: oft Kombination von Daten notwendig, z.B. Anzahl Tasks + Story Points
p.16 Lessons learned
  - wenn etwas verändert werden muss, auf das positive fokussieren
  - gemessen zu werden kann hart sein, nicht mit dem Finger zeigen
p.18 Argumente für Nein-Sager!
  Nicht jeder hat Verständnis dafür, dass seine Arbeit gemessen wird (Big Brother, Mangel an Kontrolle).
  Der Punkt hierbei ist, dass ein Team sich selbst messen sollte und nicht gemessen werden sollte.
  Niemand ist perfekt, es gibt immer etwas zu verbessen. Wer will das nicht?
  - Menschen wollen nicht "gemessen" werden: wir werden bereits überall gemessen, die Frage ist, wer liefert die Messlatte?
  - Metriken bedrohen meine Privatsphäre: die Daten entstehen sowieso im Prozess, dann sollten sie auch für Feedback genutzt werden
  - Metriken machen den Prozess schwerlastig: wenn sie was machen, dann helfen Prozesse zu verbessern
  - Metriken benötigen zu viel Zeit: es gibt bereits viele Technologien Metriken einfach zu erhalten

Chapter 2: Case Study
p.22 Die richtige Lösung finden:
  - Herausfinden was schief läuft / was will ich messen
  - Was für Metriken brauche ich dafür / wo bekomme ich die Daten her?
p.24 Figure 2.4
  Mögliche Architektur eines solchen Systems.
p.29
  Metriken können helfen, dem Management Probleme visualisiert aufzuzeigen.
  CodeFlower Diagramm kann die Komplexität eines Systems grafisch darstellen.
p.32
  Feedback Loop

Project-Tracking Systems p.37
Hier werden Aufgaben definiert und zugewiesen, Bugs verwaltet und Zeit mit Aufgaben verknüpft.
Fragen die hier heraus beantwortet werden können:
  - Wie gut versteht das Team das Projekt?
  - Wie schnell bewegt sich das Team?
  - Wie konsistent ist das Team im Erledigen von Arbeit?
Messwerte:
  - Burn Down: Anzahl erledigte Arbeit über einen Zeitpunkt - liefert einen Richtwert wo man im Sprint gerade steht (verglichen zum Commitment)
  - Velocity: ist eine relative Messung der Konsistenz erledigter Arbeit über die Zeit (Commitment/Erledigt), was folgende Dinge impliziert:
    - Wie gut ist das team im Schätzen von Arbeit?
    - Wie konsistent wird die Arbeit des Teams erledigt?
    - Wir konsistent kann das Team Commitments machen und einhalten?
  - Cumulative Flow: zeigt wie viel Arbeit nach Typ dem Team zugewiesen ist, um Bottlenecks im Prozess erkennen zu können
  - Lead Time: Zeit zwischen Start und Abschluss einer Aufgabe, vor allem bei Kanban interessant (hoher Durchsatz)
  - Bug Counts: zeigt Inkonsistenzen der Software, hängt aber von der Bug-Definition ab
Tipps für Bestmögliche Daten: p.44
  - jeder nutzt das PTS
  - Aufgaben mit möglichst vielen Daten taggen
  - Aufgaben schätzen
  - gemeinsam eine DoD erstellen
  - Aufgaben kategorisieren in "gut", "ok" und "schlecht" (wie sie erledigt wurden)
Metriken: p.52
  Es werden 4 Metriken kombiniert: Schätzungen, Volumen, Bugs, Rückfälligkeit (Recidivism)
  - Aufgaben-Volumen: ist die Anzahl an Aufgaben, wird der Schätzung gegenübergestellt
    - kann aufzeigen, dass viel ungeplante Arbeit gemacht wird (Delta zwischen Schätzung und Volumen wird kleiner)
    - zeigt die größe der Aufgaben
  - Bugs: zeigen die Qualität der Software, kann ungeplante Arbeit zeigen (Volumen sinkt, aber Bugs und StoryPoints bleiben gleich)
    - bug creation rate: Anzahl Bugs nach Erstellungsdatum
    - bug completion rate: Anzahl Bugs nach Erledigungsdatum
  - Aufgaben Bewegungen: wie oft gehen Aufgaben rückwärts im definierten Workflow, können folgende Probleme aufzeigen:
    - Kommunikationsprobleme
    - DoD nicht klar definiert
    - Release-Druck, schnell abarbeiten
  - Sortieren mit Tags und Labels: können helfen, die Daten in Kibana besser darzustellen
  
Source-Control p.62
Befindet sich nah an der Arbeit der Entwickler und kann deshalb Antworten auf folgende Fragen geben:
  - Wie viele Änderungen gibt es?
  - Wie gut arbeiten die Teams zusammen?
Und in Kombination mit dem PTS auch: Größe der Aufgaben, Genauigkeit der Schätzungen, wie viel wird erledigt.
Tipps für bestmögliche Daten p65: verteilte VCS (wie Git) und Pull Requests verwenden
Daten aus dezentralisierten SCM:
  - Commits: Nachricht, Author, Committer, Parent-Committs
  - Pull Requests: Author, Reviewer, Kommentare, Repository
  - Wer macht was (Reviewer/Author)? Wer arbeitet zusammen? Wie viel wird zusammengearbeitet?
  - Wo finden die meisten Änderungen statt? Wer ist wie oft in welchem Modul? Wer ändert am meisten Code?
Daten aus zentralisierten SCM:
  - Change: Wer und was
Daten aus SCM:
  - Wie viel Code Ändert jeder im Team?
  - Wie viele Änderungen passieren in der Codebasis?
  - Viele Beispiele der Visualisierung in Github!
Metriken:
  - SCM Daten: PRs, Abgelehnte PRs, Gemergte PRs, Commits, Reviews, Comments, CLOC
  - SCM Aktivität p78: Anzahl Pull Requests und Kommentare 
  - Abgelehnte vs Gemergte PRs (Zeichen, wie das Team zusammenarbeitet)
  - CLOC über die Zeit
  - Anzahl Merges / Pull Requests: Mögliche Aussage über Reviews => Wissensverteilung / Qualität (nicht aus dem Buch)

CI / CD p.84
Sind die Elemente im Workflow, die unter den Teams am meisten variieren können.
  - Wie schnell können Änderungen ausgeliefert werden? Wie schnell werden sie ausgeliefert?
  - Wie konsistent macht das Team seine Arbeit? Wird guter Code produziert?
Kombiniert mit PTS und SCM Daten:
  - Haben die Aufgaben angemessene Größen? Sind Schätzungen genau? Wie viel wird erledigt?
Build Pipeline kann sehr viel nützliche Daten erzeugen, vor allem mit Tools wie SonarQ (statische Analysen).
Daten aus CI/CD p.91: hängen stark vom jeweiligen Setup ab
  - Dauer und geschätzte Dauer der Builds
  - Status der Builds (erfolgreich vs fehlerhaft)
  - Build-Frequenz
  - Test Reports (über z.B. TestNG)
  - Statische Code Analysen (z.B. mit SonarQube) - z.B. Code Coverage
  - Stresstests / Benchmarking (z.B. Gatling, JMeter)
  - BDD Tests - Tests in der Form von Behaviour-driven Development
Metriken:
  - erfolgreiche Builds / fehlerhafte Builds / Testabdeckung / ausgeführte Tests
  - kombiniert: erfolgreiche Builds / fehlerhafte Builds / Commits / Kommentare / erledigte StoryPoints

Produktionssysteme p. 107
Hier können APM (appl perf monit) oder BI (busin intelli) Kennzahlen ausgelesen werden.
Wichtig, da es aussagt, wie das System arbeitet und ob die Kunden zufrieden sind damit.
BI nahe am Dev Team halten, da dadurch verstanden werden kann, wie der Kunde die App nutzt.
Kann auch durch Software (Frameworks) unterstützt werden: StatsD oder Atlas (Netflix) und Graphite zur Speicherung und Darstellung.
Daten aus APM / BI Systemen p.118:
  - Server Health: CPU Nutzung, Heap-size, Fehlerraten, Antwortzeiten
  - Nutzerverhalten: wie viele User? wie lange sind User wo? Conversion Rate (wie mache ich aus Usern Kunden)
  - Semantisches Logging: spezielle Daten, wie z.B. was suchen User auf der Seite, aber natürlich auch Fehler
Summary p.124

Metriken anwenden (Teams, Prozesse, Software)

Mit den gesammelten Daten arbeiten p.127
Metriken erstellen, 2 Dinge notwendig: Daten und eine Funktion, um die Metrik zu berechnen
Schritte, um eine Metrik zu berechnen:
  1. Erkunde deinen Daten
  2. Daten aufgliedern, um zu bestimmen was man verfolgen will
  3. Erstelle Funkionen um diese Daten
Ein Beispiel für eine solche berechnete Metrik ist die Rückfälligkeit (Recidivism) = Aufgaben die im Workflow rückwärts gehen / (vorwärts + rückwärts)
Oder Comment To Commit Ratio = Reviews / (Merged Pull Requests + Commits)
Oder Mean Time To repair = Datum wenn Bug gefunden wurde - Datum wenn Bug gelöst wurde
Die Daten nutzen um "Gut" zu definierten (es gibt 3 "Gut")
  - Gute Software: macht sie, was sie soll und ist sie gut aufgebaut
  - Gutes Team: da jedes Team individuell ist, sind auch oft die Metriken dazu individuell
  - Gute Metriken: werden verwendet, um das Team zu messen und sollten daher vertrauensvoll und konsistent sein
Subjektivität in Objektivität wandeln:
Aufgaben können vom Team mit bestimmten Tags versehen werden, um subjektive Wahrnehmungen festzuhalten.
Ein beispiel wäre "gut" oder "schlecht" für den Ablauf einer Aufgabe oder "geteilt" wenn eine Aufgaben im Team erledigt wurde.
Diese subjektiven Daten können mit passenden objektive Daten dargestellt werden, um eine bessere sicht darauf zu bekommen.
Zurückarbeiten von guten Releases:
Ein anderer Ansatz ist es, gute Releases heranzuziehen und herauszufinden, was dort anders gemacht wurde.
Wie man Metriken erstellt p.135: "A model is a formal representation of a theory."
  - auf Metriken sollte reagiert werden können
  - Metriken sollten sich nach Team-Grundsätzen und Kerngeschäften ausrichten
  - Metriken sollten für sich alleine stehen können

Technische Qualität der Software messen p.154:
Aus sicht der agilen Prinzipien: frühzeitige und wiederkehrede Auslieferung, Änderungen willkommen heißen
Die sog. -ilities: maintainab- / extens- / reliab- / avalability / security / usability
Maintainability p.158: wenn es um Wartbarkeit geht, sind MTTR und lead time die wichtigsten Größen (ab p.159)
  - MTTR: mean time to repair
  - lead Time: Definition eines neuen Features und wenn es ausgeliefert wird
  - code Coverage: LOC die von Tests abgedeckt sind
  - coding standard rules
  - wie viel code muss für features geändert werden: CLOC in Verbindung mit Aufgaben
  - bug rates
Usability p.168:
Wichtige Eigenschaften: Securiy, Userinteraktion, Scalability, Reliability
Reliability / Availability: wie viel Prozent verfügbar (Spring Health Endpoint / New Relic) / wie gut funktioniert sie (mean time betweeen failures - response time - error rate)
Security: statische Analyen, zB mit SonarQube - dynamische Analysen, zB OWASP Zed Attack Proxy
Summary p.176

Publishing Metrics p.177
Daten sollten jenen bereitgestellt werden, die diese auch positiv beeinflussen können.
Daten zu sehen, die nicht beeinflusst werden können, verursachen nur stress oder demotivieren.
Die richtigen Daten für die richtigen Personen p.178:
  - Personen sollten nur Daten sehen, die sie auch betreffen
  - der Zeitraum in dem die Daten aktualisiert werden, sollte klar sein
  Figure 9.3 p.179 zeigt für wen welche Informationen Interessant sein können.
  Development Team p.180:
    - Key Metrics aus jedem System als Defaults
    - Metriken, die aktuelle Probleme messbar machen
    - Beispiele: Tags und Labels, PRs und Commits, Task Flow und Recisivismus, Built Verhältnis
  Manager p.184:
    - Entwicklersicht wäre Micromanagement
      - eine Übersicht über die Team-Metrics sollte er aber haben
      - auch ein Vergleich von Teams sollte möglich sein, um Tendenzen erkennen zu können
    - Beispiele: Lead Time, Velocity / Volumen, Estimate Health, Commiter und PR (als Person), Tags und Labels, PR und Commits über die Zeit
    - Bei Durchschnittswerten sollten auch immer gleich die Verteilungen mitangezeigt werden (um Ausreißer darzustellen)
  Geschäftsführer p.188:
    - Interesse am Big Picture, vor allem an Daten entlang der Strategie
    - Business Metriken hier interessant (Kapitel 6)
    - Beispiele: Anzahl Releases / Features pro Release, Entwicklungskosten
  Metriken als Beweis für Änderungen verwenden:
    - Metriken können so gedreht werden, dass sie zeigen, was man zeigen will
    - immer hinterfragen, was man aussagen will und im Hinterkopf behalten, dass die Annahnem falsch sein kann
    - weitere Metriken als Gegencheck verwenden
Methoden zur Veröffentlichung p.191:
  Hierbei sollte man sich in den Grenzen des Unternehmens bewegen.
  Dashboards:
    - Zugriff innerhalb der Firma nicht einschränken, aber intern halten
    - make it customizable
    - Bewusstsein: Daten sind dort als Tool, nicht als Waffe
    - Page Tracking verwenden, um zu verstehen, wie Dashboards genutzt werden
  Email:
    - Email optional machen, aus dem Dashboard
    - minimal erforderliche Daten im Email, aber mit Links zum Dashboard
    - den richtigen Rhytmus finden, um Personen nicht zu nerven, aber oft genug zu informieren
Summary p.199

Team gegen die Agilen Prinzipien Messen
Aus den Prinzipien ergeben sich folgende Kernfragen:
  - Sind die Teams effektiv?
  - Sind die Prozesse effektiv?
  - Sind die Requirements effektiv?
  - Ist die Software effektiv?
Daraus kann folgende Gleichung abgeleitet werden: Teams + Prozesse + Requirements = Software
Ebenso können die unterschiedlichen Prinzipien an unterschiedlichen Stellen gemessen werden Table 10.0 p.204
Effektive Software:
  - Business/spezielle Metrics
  - Gesundheitsstatistiken: Fehleranzahl, CPU/Speicher, Antwortzeiten, Transaktionen, Speicherplatz, Garbage Collection, Thread Anzahl
  - Semantisches Logging
  - Usability
  - Uptime
  - MTTF
  - Maintainability
  - MTTR
  - Lead Time
Effektiver Prozess:
  - CLOC
  - Task Volume and Estimates
  - Konsistenz der Schätzungen
  - Lead Time + Dev Time + Volume = how fast / frequently you can deliver
  - Velocity
  - erfolgreiche Builds
  - MTTR
  - Bug Counts
  - Code Coverage und mehr statische Analysen
  - PTS und SCM Kommentar Counts
Effektives Team:
  - Lead Time
  - Entwicklungszeit
  - Deploy Frequency
  - erfolgreiche / fehlerhafte Builds
  - Personen die an einem Task arbeiten, bevor er "In Progress" genommen wird / Personen die danach daran arbeiten
Effektive Requirements:
  - Task Volumen / Durchschnittliche Schätzungen
  - Recidivismus / Lead time
Summary p.219



Datenmodell VCS:
Commit
  ID
  Author
  Date
  ChangedLines
  Repository
  Project
PullRequest
  ID
  Author
  Reviewer
  CreateDate
  MergeDate
  CommentCount
  Commits [ID]
