p.4: Feedback-Loop: Collect - Measure - React - Repeat
p.5: Was sind Metriken? Eine Methode etwas zu messen oder deren Ergebisse.
  Start bei: welche Metriken haben wir bereits? Häufig sind Velocity oder CLOC (Changed Lines of Code)
p.6: Probleme:
  - agile Definition von Messungen ist nicht direkt gegeben (Wie messe ich "working software"?)
  - agile fokussiert sich auf ein Produkt, nicht ein Projekt
  - Daten sind über den Entwicklungsprozess verstreut, ohne einheitliche Sicht
p.9: Data:
  - Project Mnagement: Story Points per Sprint, fertiggestellte Tasks, neue Bugs
  - Source Control: Anzahl Änderungen im Code, Reviews, Kommentare
  - Build System: statische Code Analyse, Testabdeckung, fehlerhafte Builds
  - System Überwachung
p.13 Analyzing Data:
  - Herausfinden was relevant ist: z.B. über MindMap
  - Visualisierung: oft Kombination von Daten notwendig, z.B. Anzahl Tasks + Story Points
p.16 Lessons learned
  - wenn etwas verändert werden muss, auf das positive fokussieren
  - gemessen zu werden kann hart sein, nicht mit dem Finger zeigen
p.18 Argumente für Nein-Sager!
  Nicht jeder hat Verständnis dafür, dass seine Arbeit gemessen wird (Big Brother, Mangel an Kontrolle).
  Der Punkt hierbei ist, dass ein Team sich selbst messen sollte und nicht gemessen werden sollte.
  Niemand ist perfekt, es gibt immer etwas zu verbessen. Wer will das nicht?
  - Menschen wollen nicht "gemessen" werden: wir werden bereits überall gemessen, die Frage ist, wer liefert die Messlatte?
  - Metriken bedrohen meine Privatsphäre: die Daten entstehen sowieso im Prozess, dann sollten sie auch für Feedback genutzt werden
  - Metriken machen den Prozess schwerlastig: wenn sie was machen, dann helfen Prozesse zu verbessern
  - Metriken benötigen zu viel Zeit: es gibt bereits viele Technologien Metriken einfach zu erhalten

Chapter 2: Case Study
p.22 Die richtige Lösung finden:
  - Herausfinden was schief läuft / was will ich messen
  - Was für Metriken brauche ich dafür / wo bekomme ich die Daten her?
p.24 Figure 2.4
  Mögliche Architektur eines solchen Systems.
p.29
  Metriken können helfen, dem Management Probleme visualisiert aufzuzeigen.
  CodeFlower Diagramm kann die Komplexität eines Systems grafisch darstellen.
p.32
  Feedback Loop

Project-Tracking Systems p.37
Hier werden Aufgaben definiert und zugewiesen, Bugs verwaltet und Zeit mit Aufgaben verknüpft.
Fragen die hier heraus beantwortet werden können:
  - Wie gut versteht das Team das Projekt?
  - Wie schnell bewegt sich das Team?
  - Wie konsistent ist das Team im Erledigen von Arbeit?
Messwerte:
  - Burn Down: Anzahl erledigte Arbeit über einen Zeitpunkt - liefert einen Richtwert wo man im Sprint gerade steht (verglichen zum Commitment)
  - Velocity: ist eine relative Messung der Konsistenz erledigter Arbeit über die Zeit (Commitment/Erledigt), was folgende Dinge impliziert:
    - Wie gut ist das team im Schätzen von Arbeit?
    - Wie konsistent wird die Arbeit des Teams erledigt?
    - Wir konsistent kann das Team Commitments machen und einhalten?
  - Cumulative Flow: zeigt wie viel Arbeit nach Typ dem Team zugewiesen ist, um Bottlenecks im Prozess erkennen zu können
  - Lead Time: Zeit zwischen Start und Abschluss einer Aufgabe, vor allem bei Kanban interessant (hoher Durchsatz)
  - Bug Counts: zeigt Inkonsistenzen der Software, hängt aber von der Bug-Definition ab
Tipps für Bestmögliche Daten: p.44
  - jeder nutzt das PTS
  - Aufgaben mit möglichst vielen Daten taggen
  - Aufgaben schätzen
  - gemeinsam eine DoD erstellen
  - Aufgaben kategorisieren in "gut", "ok" und "schlecht" (wie sie erledigt wurden)
Metriken: p.52
  Es werden 4 Metriken kombiniert: Schätzungen, Volumen, Bugs, Rückfälligkeit (Recidivism)
  - Aufgaben-Volumen: ist die Anzahl an Aufgaben, wird der Schätzung gegenübergestellt
    - kann aufzeigen, dass viel ungeplante Arbeit gemacht wird (Delta zwischen Schätzung und Volumen wird kleiner)
    - zeigt die größe der Aufgaben
  - Bugs: zeigen die Qualität der Software, kann ungeplante Arbeit zeigen (Volumen sinkt, aber Bugs und StoryPoints bleiben gleich)
    - bug creation rate: Anzahl Bugs nach Erstellungsdatum
    - bug completion rate: Anzahl Bugs nach Erledigungsdatum
  - Aufgaben Bewegungen: wie oft gehen Aufgaben rückwärts im definierten Workflow, können folgende Probleme aufzeigen:
    - Kommunikationsprobleme
    - DoD nicht klar definiert
    - Release-Druck, schnell abarbeiten
  - Sortieren mit Tags und Labels: können helfen, die Daten in Kibana besser darzustellen
  
Source-Control p.62
Befindet sich nah an der Arbeit der Entwickler und kann deshalb Antworten auf folgende Fragen geben:
  - Wie viele Änderungen gibt es?
  - Wie gut arbeiten die Teams zusammen?
Und in Kombination mit dem PTS auch: Größe der Aufgaben, Genauigkeit der Schätzungen, wie viel wird erledigt.
Tipps für bestmögliche Daten p65: verteilte VCS (wie Git) und Pull Requests verwenden
Daten aus dezentralisierten SCM:
  - Commits: Nachricht, Author, Committer, Parent-Committs
  - Pull Requests: Author, Reviewer, Kommentare, Repository
  - Wer macht was (Reviewer/Author)? Wer arbeitet zusammen? Wie viel wird zusammengearbeitet?
  - Wo finden die meisten Änderungen statt? Wer ist wie oft in welchem Modul? Wer ändert am meisten Code?
Daten aus zentralisierten SCM:
  - Change: Wer und was
Daten aus SCM:
  - Wie viel Code Ändert jeder im Team?
  - Wie viele Änderungen passieren in der Codebasis?
  - Viele Beispiele der Visualisierung in Github!
Metriken:
  - SCM Daten: PRs, Abgelehnte PRs, Gemergte PRs, Commits, Reviews, Comments, CLOC
  - SCM Aktivität p78: Anzahl Pull Requests und Kommentare 
  - Abgelehnte vs Gemergte PRs (Zeichen, wie das Team zusammenarbeitet)
  - CLOC über die Zeit
  - Anzahl Merges / Pull Requests: Mögliche Aussage über Reviews => Wissensverteilung / Qualität (nicht aus dem Buch)

CI / CD p.84
Sind die Elemente im Workflow, die unter den Teams am meisten variieren können.
  - Wie schnell können Änderungen ausgeliefert werden? Wie schnell werden sie ausgeliefert?
  - Wie konsistent macht das Team seine Arbeit? Wird guter Code produziert?
Kombiniert mit PTS und SCM Daten:
  - Haben die Aufgaben angemessene Größen? Sind Schätzungen genau? Wie viel wird erledigt?
Build Pipeline kann sehr viel nützliche Daten erzeugen, vor allem mit Tools wie SonarQ (statische Analysen).
Daten aus CI/CD p.91: hängen stark vom jeweiligen Setup ab
  - Dauer und geschätzte Dauer der Builds
  - Status der Builds (erfolgreich vs fehlerhaft)
  - Build-Frequenz
  - Test Reports (über z.B. TestNG)
  - Statische Code Analysen (z.B. mit SonarQube) - z.B. Code Coverage
  - Stresstests / Benchmarking (z.B. Gatling, JMeter)
  - BDD Tests - Tests in der Form von Behaviour-driven Development
Metriken:
  - erfolgreiche Builds / fehlerhafte Builds / Testabdeckung / ausgeführte Tests
  - kombiniert: erfolgreiche Builds / fehlerhafte Builds / Commits / Kommentare / erledigte StoryPoints

Produktionssysteme p. 107
Hier können APM (appl perf monit) oder BI (busin intelli) Kennzahlen ausgelesen werden.
Wichtig, da es aussagt, wie das System arbeitet und ob die Kunden zufrieden sind damit.
BI nahe am Dev Team halten, da dadurch verstanden werden kann, wie der Kunde die App nutzt.
Kann auch durch Software (Frameworks) unterstützt werden: StatsD oder Atlas (Netflix) und Graphite zur Speicherung und Darstellung.
Daten aus APM / BI Systemen p.118:
 - Server Health: CPU Nutzung, Heap-size, Fehlerraten, Antwortzeiten
 - Nutzerverhalten: wie viele User? wie lange sind User wo? Conversion Rate (wie mache ich aus Usern Kunden)
 - Semantisches Logging: spezielle Daten, wie z.B. was suchen User auf der Seite, aber natürlich auch Fehler
 Summary p.124

 
